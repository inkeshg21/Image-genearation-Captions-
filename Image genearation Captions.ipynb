{"cells":[{"cell_type":"markdown","metadata":{"id":"0-qHOt2zvMvQ"},"source":["# Image genearation Captions\n","## Introduction\n","This assignment aims to describe the content of an image by using CNNs and RNNs to build an Image Caption Generator. The model would be based on Tensorflow and Keras. The dataset used is Flickr 8K [5], consisting of 8,000 images each one paired with five different captions to provide clear descriptions. \n","\n","The model architectures consists of a CNN which extracts the features and encodes the input image and a Recurrent Neural Network (RNN) based on Long Short Term Memory (LSTM) layers. The most significant difference with other models is that the image embedding is provided as the first input to the RNN network and only once."]},{"cell_type":"markdown","metadata":{"id":"bt3hHGrEwcFm"},"source":["## Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"gfjrnYN0vLpa"},"outputs":[],"source":["import re\n","import random\n","\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from tensorflow import keras\n","from time import time\n","\n","from tqdm import tqdm # progress bar\n","from sklearn.model_selection import train_test_split # Dividing train test\n","from nltk.translate.bleu_score import corpus_bleu # BLEU Score"]},{"cell_type":"markdown","metadata":{"id":"0w6rfmajwbEk"},"source":["## Dataset\n","Load dataset from local path or google drive"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JwKGCUZlxwEz"},"outputs":[],"source":["# Change this path to the dataset downloaded from Flickr8 [5]\n","dataset_path = \"E:/Project/Image genearation Captions/Resources\"\n","dataset_images_path = dataset_path + \"/Images/\" "]},{"cell_type":"markdown","metadata":{"id":"93wtzSn0MbAz"},"source":["Images configuration"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fOxSmplF3Jdc"},"outputs":[],"source":["img_height = 180\n","img_width = 180\n","validation_split = 0.2"]},{"cell_type":"markdown","metadata":{"id":"J9js_Vnr3VTN"},"source":["### Encoder Model\n","\n","In order to extract the features from the images, a pretrained CNN model, named Inception V3 was used. In the figure below, there is the representation of the architecture of the used network.\n","\n","![Inception Architecture](https://paperswithcode.com/media/methods/inceptionv3onc--oview_vjAbOfw.png)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2mRYvH-lKjge"},"outputs":[],"source":["# Remove the last layer of the Inception V3 model\n","def get_encoder():\n","    image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n","    new_input = image_model.input\n","    hidden_layer = image_model.layers[-1].output\n","\n","    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","    return image_features_extract_model"]},{"cell_type":"markdown","metadata":{"id":"MUp3k4tv3QUK"},"source":["### Read captions\n","Create dictionary with picture filename as the key and an array of captions as the value"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZmbJa3Q-Kjgb"},"outputs":[],"source":["# Preprocess the caption, splitting the string and adding <start> and <end> tokens\n","def get_preprocessed_caption(caption):    \n","    caption = re.sub(r'\\s+', ' ', caption)\n","    caption = caption.strip()\n","    caption = \"<start> \" + caption + \" <end>\"\n","    return caption"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"inAwp6UWw42K"},"outputs":[],"source":["images_captions_dict = {}\n","\n","with open(dataset_path + \"/captions.txt\", \"r\") as dataset_info:\n","    next(dataset_info) # Omit header: image, caption\n","\n","    # Using a subset of 4,000 entries out of 40,000\n","    for info_raw in list(dataset_info)[:4000]:\n","        info = info_raw.split(\",\")\n","        image_filename = info[0]\n","        caption = get_preprocessed_caption(info[1])\n","\n","        if image_filename not in images_captions_dict.keys():\n","            images_captions_dict[image_filename] = [caption]\n","        else:\n","            images_captions_dict[image_filename].append(caption)"]},{"cell_type":"markdown","metadata":{"id":"xF_Jv1AKKjgf"},"source":["### Read images\n","Create dictionary with image filename as key and the image feature extracted using the pretrained model as the value."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"MLtoBOSwNnr7"},"outputs":[],"source":["def load_image(image_path):\n","    img = tf.io.read_file(dataset_images_path + image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (img_height, img_width))\n","    img = tf.keras.applications.inception_v3.preprocess_input(img) # preprocessing needed for pre-trained model\n","    return img, image_path"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"LtV_P5lY7K-x"},"outputs":[],"source":["image_captions_dict_keys = list(images_captions_dict.keys())\n","image_dataset = tf.data.Dataset.from_tensor_slices(image_captions_dict_keys)\n","image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jO95FUrAKjgs","outputId":"3052b66e-4015-4137-c154-4fcae017226b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 13/13 [00:20<00:00,  1.60s/it]\n"]}],"source":["images_dict = {}\n","encoder = get_encoder()\n","for img_tensor, path_tensor in tqdm(image_dataset):\n","    batch_features_tensor = encoder(img_tensor)\n","    \n","    # Loop over batch to save each element in images_dict\n","    for batch_features, path in zip(batch_features_tensor, path_tensor):\n","        decoded_path = path.numpy().decode(\"utf-8\")\n","        images_dict[decoded_path] = batch_features.numpy()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"image-captioning.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
